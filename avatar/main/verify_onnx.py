import torch
import json
import argparse
import os.path as osp
import numpy as np
import onnxruntime # è¼‰å…¥ ONNX åŸ·è¡Œç’°å¢ƒ
#python verify_onnx.py --subject_id gyeongsik --test_epoch 4 --motion_path /home/cgvmis418/ExAvatar_to_Unity/motions/jungkook_standing_next_to_you --onnx_path "../data/NeuMan/data/gyeongsik/human_model_ChunkedGroupNorm_lbs.onnx"

# æ–°å¢å¿…è¦çš„ importï¼Œèˆ‡ export_onnx.py åŒæ­¥
from pytorch3d.transforms import matrix_to_quaternion
from pytorch3d.ops import knn_points

# å‡è¨­æ‚¨çš„ config, base, model, smpl_x æ¨¡çµ„éƒ½åœ¨å¯å°å…¥çš„è·¯å¾‘ä¸­
from config import cfg
from base import Tester
from utils.smpl_x import smpl_x
from model import get_model
from plyfile import PlyData, PlyElement
# --- æ–°å¢è¼”åŠ©å‡½å¼ ---
def rgb_to_sh(rgb):
    """
    å°‡ [0, 1] ç¯„åœçš„ RGB é¡è‰²è½‰æ›ç‚º 0 éšçƒè«§å‡½æ•¸ (Spherical Harmonics) ä¿‚æ•¸ã€‚
    é€™æ˜¯ 3DGS è«–æ–‡ä¸­ SH2RGB çš„é€†æ“ä½œã€‚ C0 = 0.28209479177387814
    """
    C0 = 0.28209479177387814
    return (rgb - 0.5) / C0

# --- ä¿®æ”¹å¾Œçš„å„²å­˜å‡½å¼ ---
def save_to_ply_3dgs_format(path, mean_3d, rgb, raw_opacity, raw_scale, rotation):
    """
    å°‡æ¨¡å‹åƒæ•¸å„²å­˜ç‚ºåŸå§‹ 3DGS è«–æ–‡ä¸­å®šç¾©çš„ .ply æ ¼å¼ã€‚
    é€™å€‹æ ¼å¼å„²å­˜çš„æ˜¯æœªç¶“å•Ÿå‹•å‡½æ•¸è™•ç†çš„åŸå§‹å¯è¨“ç·´åƒæ•¸ã€‚

    Args:
        path (str): å„²å­˜ .ply æª”æ¡ˆçš„è·¯å¾‘.
        mean_3d (np.array): Gaussians çš„ä¸­å¿ƒé»ä½ç½® (N, 3).
        rgb (np.array): Gaussians çš„é¡è‰² (N, 3), ç¯„åœæ‡‰åœ¨ [0, 1].
        raw_opacity (np.array): æœªç¶“ sigmoid è™•ç†çš„åŸå§‹ logit opacity (N, 1).
        raw_scale (np.array): æœªç¶“ exp() è™•ç†çš„åŸå§‹ log-space scale (N, 3).
        rotation (np.array): æ—‹è½‰å››å…ƒæ•¸ (N, 4).
    """
    print(f"æ­£åœ¨å°‡è¼¸å‡ºå„²å­˜ç‚ºåŸå§‹ 3DGS æ ¼å¼è‡³ '{path}'...")
    
    num_points = mean_3d.shape[0]

    # 1. è™•ç†é¡è‰²: å°‡ RGB è½‰æ›ç‚º f_dc (0éš SH)ã€‚f_rest è¨­ç‚º 0ã€‚
    features_dc = rgb_to_sh(rgb).astype(np.float32)
    # å‡è¨­ SH degree ç‚º 3ï¼Œå‰‡ rest features æœ‰ 15*3=45 å€‹
    # å¦‚æœæ‚¨çš„æ¨¡å‹ä¸éœ€è¦é€™éº¼é«˜çš„éšæ•¸ï¼Œå¯ä»¥è¨­ç‚ºæ›´å°çš„å€¼ï¼Œä½†ç‚ºäº†å…¼å®¹æ€§ï¼Œé€™è£¡ä½¿ç”¨ 45
    features_rest = np.zeros((num_points, 45), dtype=np.float32)

    # ç¢ºä¿å…¶ä»–åƒæ•¸æ˜¯ float32
    xyz = mean_3d.astype(np.float32)
    opacity = np.clip(raw_opacity, 1e-6, 1.0 - 1e-6)
    opacities = np.log(opacity / (1.0 - opacity))
    scales = np.log(raw_scale).astype(np.float32)
    rotations = rotation.astype(np.float32)
    
    # å»ºç«‹æ‰€æœ‰å±¬æ€§çš„åˆ—è¡¨
    dtype_full = [
        ('x', 'f4'), ('y', 'f4'), ('z', 'f4'),
        ('nx', 'f4'), ('ny', 'f4'), ('nz', 'f4'),
        ('f_dc_0', 'f4'), ('f_dc_1', 'f4'), ('f_dc_2', 'f4')
    ]
    # æ·»åŠ  f_rest å±¬æ€§
    for i in range(features_rest.shape[1]):
        dtype_full.append((f'f_rest_{i}', 'f4'))
    
    dtype_full.extend([
        ('opacity', 'f4'),
        ('scale_0', 'f4'), ('scale_1', 'f4'), ('scale_2', 'f4'),
        ('rot_0', 'f4'), ('rot_1', 'f4'), ('rot_2', 'f4'), ('rot_3', 'f4')
    ])

    # æº–å‚™å¯«å…¥æª”æ¡ˆçš„è³‡æ–™
    elements = np.empty(num_points, dtype=dtype_full)
    normals = np.zeros_like(xyz, dtype=np.float32)
    
    attributes = np.concatenate((
        xyz,
        normals,
        features_dc,
        features_rest,
        opacities,
        scales,
        rotations
    ), axis=1)
    
    elements[:] = list(map(tuple, attributes))

    # å»ºç«‹ PlyData ç‰©ä»¶ä¸¦å¯«å…¥æª”æ¡ˆ
    el = PlyElement.describe(elements, 'vertex')
    PlyData([el]).write(path)
    
    print(f"âœ… æˆåŠŸä»¥åŸå§‹ 3DGS æ ¼å¼å„²å­˜ {num_points} å€‹ Gaussians è‡³ '{path}'ã€‚")


# --- æ­¥é©Ÿ 1: ä½¿ç”¨èˆ‡ export_onnx.py å®Œå…¨ç›¸åŒçš„ ModelWrapper ---
class ModelWrapper(torch.nn.Module):
    def __init__(self, model):
        super().__init__()
        self.model = model
        with torch.no_grad():
            mesh_neutral_pose, mesh_neutral_pose_wo_upsample, _, transform_mat_neutral_pose = model.module.human_gaussian.get_neutral_pose_human(jaw_zero_pose=True, use_id_info=True)
            joint_zero_pose = model.module.human_gaussian.get_zero_pose_human()

            # extract triplane feature
            tri_feat = model.module.human_gaussian.extract_tri_feature()
        
            # get Gaussian assets
            geo_feat = model.module.human_gaussian.geo_net(tri_feat)
            mean_offset = model.module.human_gaussian.mean_offset_net(geo_feat) # mean offset of Gaussians
            scale = model.module.human_gaussian.scale_net(geo_feat) # scale of Gaussians
            rgb = model.module.human_gaussian.rgb_net(tri_feat) # rgb of Gaussians
            mean_3d = mesh_neutral_pose + mean_offset # å¤§ pose
        # --- æ ¸å¿ƒä¿®æ”¹ï¼šå°‡å‚³å…¥çš„å¸¸æ•¸å¼µé‡è¨»å†Šç‚º buffer ---
        self.register_buffer('tri_feat', tri_feat)
        self.register_buffer('scale', scale)
        self.register_buffer('rgb', rgb)
        self.register_buffer('mean_3d', mean_3d)
        self.register_buffer('joint_zero_pose', joint_zero_pose)
        self.register_buffer('mesh_neutral_pose_wo_upsample', mesh_neutral_pose_wo_upsample)
        self.register_buffer('transform_mat_neutral_pose', transform_mat_neutral_pose)
        self.register_buffer('parents', model.module.human_gaussian.smplx_layer.parents)
        self.register_buffer('skinning_weight', model.module.human_gaussian.skinning_weight)
        
        # æ ¹æ“š smplx_params_smoothed_0.json å’Œ cam_params_0.json çš„çµæ§‹ï¼Œå®šç¾©è¼¸å…¥å¼µé‡çš„éµåå’Œé †åº
        # **é€™å€‹é †åºå¿…é ˆèˆ‡å¾Œé¢å»ºç«‹ dummy_inputs çš„é †åºå®Œå…¨ä¸€è‡´**
        self.smplx_keys = [
             'body_pose', 'jaw_pose', 'leye_pose', 'reye_pose', 
            'lhand_pose', 'rhand_pose', 'expr'
        ]
        # æ³¨æ„: cam_params_0.json ä¸­çš„ 't' åœ¨ module.py ä¸­è¢«ç•¶ä½œ cam_param['t'] ä½¿ç”¨ï¼Œ
        # ä½† smplx_params_smoothed_0.json ä¸­ä¹Ÿæœ‰ 'trans'ã€‚ç‚ºé¿å…æ··æ·†ï¼Œè«‹ç¢ºèªæ‚¨çš„æ¨¡å‹ç¢ºå¯¦å¦‚æ­¤ä½¿ç”¨ã€‚
        # æ ¹æ“š cam_params_0.json çš„å…§å®¹ï¼Œé€™è£¡çš„éµæ‡‰ç‚º 'R', 't', 'focal', 'princpt'ã€‚
        self.cam_keys = []

    def forward(self, *inputs):
        # å°‡å‚³å…¥çš„æ‰å¹³åŒ–å¼µé‡å…ƒçµ„ (tuple of tensors) é‡æ–°çµ„åˆæˆå­—å…¸
        smplx_param = {}
        cam_param = {}
        
        smplx_input_count = len(self.smplx_keys)
        cam_inputs_count = len(self.cam_keys)
        smplx_inputs_tuple = inputs[:smplx_input_count]
        cam_inputs_tuple = inputs[smplx_input_count:smplx_input_count+cam_inputs_count]
        # joint_zero_pose = self.model.module.human_gaussian.get_zero_pose_human()

        for i, key in enumerate(self.smplx_keys):
            smplx_param[key] = smplx_inputs_tuple[i]
            
        for i, key in enumerate(self.cam_keys):
            cam_param[key] = cam_inputs_tuple[i]

        # å‘¼å«åŸå§‹æ¨¡å‹çš„ human_gaussian éƒ¨åˆ†
 
        # get pose-dependent Gaussian assets
        mean_offset_offset, scale_offset = self.model.module.human_gaussian.forward_geo_network(self.tri_feat, smplx_param)
        scale, scale_refined = torch.exp(self.scale).repeat(1,3), torch.exp(self.scale+scale_offset).repeat(1,3)
        mean_combined_offset, mean_offset_offset = self.model.module.human_gaussian.get_mean_offset_offset(smplx_param, mean_offset_offset)
        mean_3d_refined = self.mean_3d + mean_combined_offset # å¤§ pose

        # smplx facial expression offset
        smplx_expr_offset = (smplx_param['expr'][None,None,:] * self.model.module.human_gaussian.expr_dirs).sum(2)
        mean_3d = self.mean_3d + smplx_expr_offset # å¤§ pose
        mean_3d_refined = mean_3d_refined + smplx_expr_offset # å¤§ pose

        # get nearest vertex
        # for hands and face, assign original vertex index to use sknning weight of the original vertex
        nn_vertex_idxs = knn_points(mean_3d[None,:,:], self.mesh_neutral_pose_wo_upsample[None,:,:], K=1, return_nn=True).idx[0,:,0] # dimension: smpl_x.vertex_num_upsampled
        nn_vertex_idxs = self.model.module.human_gaussian.lr_idx_to_hr_idx(nn_vertex_idxs)
        mask = (self.model.module.human_gaussian.is_rhand + self.model.module.human_gaussian.is_lhand + self.model.module.human_gaussian.is_face) > 0
        updates = torch.arange(smpl_x.vertex_num_upsampled, device=nn_vertex_idxs.device, dtype=torch.int64)
        nn_vertex_idxs = torch.where(mask, updates, nn_vertex_idxs)

        # get transformation matrix of the nearest vertex and perform lbs
        # transform_mat_joint = self.model.module.human_gaussian.get_transform_mat_joint(self.transform_mat_neutral_pose, joint_zero_pose, smplx_param)
        # transform_mat_vertex = self.model.module.human_gaussian.get_transform_mat_vertex(transform_mat_joint, nn_vertex_idxs)
        # mean_3d = self.model.module.human_gaussian.lbs(mean_3d, transform_mat_vertex, smplx_param['trans']) # posed with smplx_param
        # mean_3d_refined = self.model.module.human_gaussian.lbs(mean_3d_refined, transform_mat_vertex, smplx_param['trans']) # posed with smplx_param
        
        # forward to rgb network
        rgb = (torch.tanh(self.rgb) + 1) / 2
        
        rotation = matrix_to_quaternion(torch.eye(3).float().cuda()[None,:,:].repeat(smpl_x.vertex_num_upsampled,1,1)) # constant rotation
        opacity = torch.ones((smpl_x.vertex_num_upsampled,1)).float().cuda() # constant opacity
        # æ ¹æ“š module.py çš„å®šç¾©ï¼Œhuman_asset æ˜¯ä¸€å€‹å­—å…¸ã€‚
        # ONNX å°å‡ºéœ€è¦è¿”å›ä¸€å€‹å¼µé‡æˆ–å¼µé‡çš„å…ƒçµ„ï¼Œå› æ­¤æˆ‘å€‘æå–å­—å…¸ä¸­çš„æ‰€æœ‰å¼µé‡ã€‚
        return (
            mean_3d,
            opacity,
            scale,
            rotation, 
            rgb,
            mean_3d_refined,
            scale_refined,
            self.joint_zero_pose,
            self.transform_mat_neutral_pose,
            # nn_vertex_idxs,
            self.parents,
            self.skinning_weight[nn_vertex_idxs,:]
        )

def main():
    parser = argparse.ArgumentParser(description="Verify ONNX model against PyTorch model and export to PLY")
    parser.add_argument('--subject_id', type=str, required=True, help="Subject ID")
    parser.add_argument('--test_epoch', type=str, required=True, help="Model checkpoint epoch")
    parser.add_argument('--motion_path', type=str, required=True, help="Path to motion data")
    parser.add_argument('--onnx_path', type=str, default='human_gaussian_model.onnx', help="Path to the ONNX model to verify")
    args = parser.parse_args()

    cfg.set_args(args.subject_id)

    print("æ­£åœ¨è¼‰å…¥ PyTorch æ¨¡å‹ä¸¦æº–å‚™ç¯„ä¾‹è¼¸å…¥...")
    tester = Tester(args.test_epoch)
    
    root_path = osp.join('..', 'data', cfg.dataset, 'data', cfg.subject_id)
    with open(osp.join(root_path, 'smplx_optimized', 'shape_param.json')) as f:
        shape_param = torch.FloatTensor(json.load(f))
    with open(osp.join(root_path, 'smplx_optimized', 'face_offset.json')) as f:
        face_offset = torch.FloatTensor(json.load(f))
    with open(osp.join(root_path, 'smplx_optimized', 'joint_offset.json')) as f:
        joint_offset = torch.FloatTensor(json.load(f))
    with open(osp.join(root_path, 'smplx_optimized', 'locator_offset.json')) as f:
        locator_offset = torch.FloatTensor(json.load(f))
    smpl_x.set_id_info(shape_param, face_offset, joint_offset, locator_offset)
    
    tester.smplx_params = None
    tester._make_model()
    tester.model.eval()

    # *** å»ºè­°ï¼šä½¿ç”¨èˆ‡ export_onnx.py ç›¸åŒçš„ frame_idx ä»¥ç¢ºä¿è¼¸å…¥å®Œå…¨ä¸€è‡´ ***
    frame_idx = 100
    cam_param_file = osp.join(args.motion_path, 'cam_params', f'{frame_idx}.json')
    smplx_param_file = osp.join(args.motion_path, 'smplx_optimized', 'smplx_params_smoothed', f'{frame_idx}.json')

    with open(cam_param_file) as f:
        cam_param_dict = {k: torch.FloatTensor(v).cuda() for k, v in json.load(f).items()}
    with open(smplx_param_file) as f:
        smplx_param_dict = {k: torch.FloatTensor(v).cuda().view(-1) for k, v in json.load(f).items()}

    # --- æ­¥é©Ÿ 2: æ›´æ–°æ¨¡å‹åˆå§‹åŒ–å’Œè¼¸å…¥/è¼¸å‡ºåˆ—è¡¨ ---
    wrapped_model = ModelWrapper(tester.model).cuda().eval()
    
    smplx_inputs_tuple = tuple(smplx_param_dict[key] for key in wrapped_model.smplx_keys)
    cam_inputs_tuple = tuple(cam_param_dict[key] for key in wrapped_model.cam_keys)
    dummy_inputs = smplx_inputs_tuple + cam_inputs_tuple
    
    input_names = wrapped_model.smplx_keys + wrapped_model.cam_keys
    # æ›´æ–°è¼¸å‡ºçš„åç¨±åˆ—è¡¨ä»¥åŒ¹é… ModelWrapper çš„å›å‚³å€¼
    output_names = [
        'mean_3d',
            'opacity',
            'scale',
            'rotation', 
            'rgb',
            'mean_3d_refined',
            'scale_refined',
            'joint_zero_pose',
            'transform_mat_neutral_pose',
            'parents',
            'skinning_weight'
    ]
    print("PyTorch æ¨¡å‹èˆ‡è¼¸å…¥æº–å‚™å®Œæˆã€‚")

    # --- æ­¥é©Ÿ 3: åŸ·è¡Œ PyTorch æ¨¡å‹æ¨è«– ---
    print("\næ­£åœ¨åŸ·è¡Œ PyTorch æ¨¡å‹æ¨è«–...")
    with torch.no_grad():
        pytorch_outputs = wrapped_model(*dummy_inputs)
    pytorch_outputs_np = [t.cpu().numpy() for t in pytorch_outputs]
    print("PyTorch æ¨è«–å®Œæˆã€‚")

    # --- æ­¥é©Ÿ 4: è¼‰å…¥ ONNX æ¨¡å‹ä¸¦åŸ·è¡Œæ¨è«– ---
    print(f"\næ­£åœ¨è¼‰å…¥ ONNX æ¨¡å‹ '{args.onnx_path}' ä¸¦åŸ·è¡Œæ¨è«–...")
    ort_session = onnxruntime.InferenceSession(args.onnx_path)
    ort_inputs = {
        input_name: input_tensor.cpu().numpy()
        for input_name, input_tensor in zip(input_names, dummy_inputs)
    }
    onnx_outputs = ort_session.run(output_names, ort_inputs)
    print("ONNX æ¨è«–å®Œæˆã€‚")

    # --- æ­¥é©Ÿ 5: æ¯”è¼ƒå…©å€‹æ¨¡å‹çš„è¼¸å‡º ---
    print("\n--- è¼¸å‡ºçµæœæ¯”è¼ƒ ---")
    TOLERANCE = 1e-4
    all_match = True

    for i in range(len(output_names)):
        pytorch_res = pytorch_outputs_np[i]
        onnx_res = onnx_outputs[i]
        output_name = output_names[i]

        print(f"\n--- è¼¸å‡º '{output_name}' ---")
        if pytorch_res.shape != onnx_res.shape:
            print(f"   ç‹€æ…‹: âŒ å½¢ç‹€ä¸åŒ¹é…!")
            print(f"   PyTorch shape: {pytorch_res.shape}")
            print(f"   ONNX shape:    {onnx_res.shape}")
            all_match = False
            continue

        abs_diff = np.abs(pytorch_res - onnx_res)
        max_diff = np.max(abs_diff)
        mean_diff = np.mean(abs_diff)
        
        num_elements = pytorch_res.size
        outlier_count = np.sum(abs_diff > TOLERANCE)
        error_ratio = outlier_count / num_elements
        
        is_close = outlier_count == 0

        if is_close:
            status_icon = "âœ…"
            status_text = "é©—è­‰é€šé"
            all_match = all_match and True
        else:
            status_icon = "âŒ"
            status_text = "é©—è­‰å¤±æ•—"
            all_match = False
        
        print(f"   ç‹€æ…‹: {status_icon} {status_text}")
        print(f"   æœ€å¤§çµ•å°èª¤å·®: {max_diff:.6g}")
        print(f"   å¹³å‡çµ•å°èª¤å·® (MAE): {mean_diff:.6g}")
        print(f"   èª¤å·® > {TOLERANCE} çš„å…ƒç´ æ•¸é‡: {outlier_count} / {num_elements}")
        print(f"   å‡ºéŒ¯æ¯”ä¾‹: {error_ratio:.4%}")
    
    print("\n\n--- é©—è­‰ç¸½çµ ---")
    if all_match:
        print("ğŸ‰ æ‰€æœ‰è¼¸å‡ºå‡åœ¨å®¹å¿åº¦å…§ï¼ONNX æ¨¡å‹å·²æˆåŠŸé©—è­‰ã€‚")
    else:
        print("ğŸ’” ç™¼ç¾éƒ¨åˆ†è¼¸å‡ºä¸åŒ¹é…ã€‚è«‹æ ¹æ“šä¸Šè¿°è©³ç´°æŒ‡æ¨™é€²è¡Œè©•ä¼°ã€‚")

    # --- NEW CODE ---
    # åœ¨é©—è­‰çµæŸå¾Œï¼Œå°‡ PyTorch å’Œ ONNX çš„è¼¸å‡ºå„²å­˜ç‚º .ply æª”æ¡ˆ
    print("\n\n--- å„²å­˜ 3DGS PLY æª”æ¡ˆ ---")

    # å„²å­˜ PyTorch æ¨¡å‹çš„è¼¸å‡º
    save_to_ply_3dgs_format(
        path='output_pytorch.ply',
        mean_3d=pytorch_outputs_np[output_names.index('mean_3d')],
        rgb=pytorch_outputs_np[output_names.index('rgb')],
        raw_opacity=pytorch_outputs_np[output_names.index('opacity')],
        raw_scale=pytorch_outputs_np[output_names.index('scale')],
        rotation=pytorch_outputs_np[output_names.index('rotation')]
    )
    
    # å„²å­˜ ONNX æ¨¡å‹çš„è¼¸å‡º
    save_to_ply_3dgs_format(
        path='output_onnx.ply',
        mean_3d=onnx_outputs[output_names.index('mean_3d')],
        rgb=onnx_outputs[output_names.index('rgb')],
        raw_opacity=onnx_outputs[output_names.index('opacity')],
        raw_scale=onnx_outputs[output_names.index('scale')],
        rotation=onnx_outputs[output_names.index('rotation')]
    )


if __name__ == "__main__":
    main()